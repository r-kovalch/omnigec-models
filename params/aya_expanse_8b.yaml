baseline:
  temperature: 0.3
  top_p: 0.75
  top_k: 0
  max_new_tokens: 700
training:
  num_train_epochs: 12
  per_device_train_batch_size: 5
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8
  gradient_checkpointing: true
  optim: "paged_adamw_32bit"
  save_steps: 25
  logging_steps: 10
  learning_rate: 3e-5
  weight_decay: 0.0
  max_grad_norm: 1.0
  fp16: false
  bf16: true
  warmup_steps: 50
  group_by_length: false
  lr_scheduler_type: "cosine"
  report_to: "wandb"
  eval_strategy: "steps"
  save_strategy: "steps"
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  save_total_limit: 1
  load_best_model_at_end: true
  eval_steps: 25
early_stopping:
  early_stopping_patience: 75
lora:
  lora_alpha: 128
  r: 64
  bias: "none"
  task_type: "CAUSAL_LM"
  target_modules:
  - "q_proj"
  - "v_proj"
  - "k_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
