baseline:
  temperature: 1.0
  top_p: 0.95
  top_k: 64
  max_new_tokens: 700
training:
  num_train_epochs: 12
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8
  gradient_checkpointing: true
  optim: "adamw_torch_fused"
  save_steps: 100
  logging_steps: 10
  learning_rate: 3e-5
  weight_decay: 0.0
  max_grad_norm: 1.0
  fp16: false
  bf16: true
  warmup_steps: 70
  group_by_length: false
  lr_scheduler_type: "cosine"
  report_to: "wandb"
  eval_strategy: "steps"
  save_strategy: "steps"
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  save_total_limit: 1
  load_best_model_at_end: true
  eval_steps: 25
early_stopping:
  early_stopping_patience: 200
lora:
  lora_alpha: 128
  r: 64
  bias: "none"
  task_type: "CAUSAL_LM"
  target_modules: "all-linear"
  modules_to_save:
    - "lm_head"
    - "embed_tokens"
